{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d87d683-695a-4bbc-b62a-07675e1e3bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b33c31b-00a8-43a7-83f0-6061f7780f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../preprocessing/md_encoded_categorical_subset20.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cb9fcf4-6c82-4ff8-a2a9-7ca33cf0691e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building_climate_group\n",
      "Mercantile_Mixed-Humid               857\n",
      "Mercantile_Cold                      780\n",
      "Warehouse and Storage_Mixed-Humid    509\n",
      "Office_Mixed-Humid                   504\n",
      "Mercantile_Hot-Humid                 446\n",
      "Warehouse and Storage_Cold           436\n",
      "Office_Cold                          428\n",
      "Warehouse and Storage_Hot-Humid      387\n",
      "Office_Hot-Humid                     344\n",
      "Mercantile_Hot-Dry                   237\n",
      "Warehouse and Storage_Hot-Dry        221\n",
      "Food Service_Cold                    168\n",
      "Food Service_Mixed-Humid             141\n",
      "Office_Hot-Dry                       136\n",
      "Food Service_Hot-Humid               113\n",
      "Education_Mixed-Humid                 91\n",
      "Mercantile_Marine                     72\n",
      "Education_Cold                        67\n",
      "Warehouse and Storage_Marine          64\n",
      "Food Service_Hot-Dry                  55\n",
      "Office_Marine                         53\n",
      "Lodging_Cold                          39\n",
      "Education_Hot-Humid                   35\n",
      "Lodging_Hot-Humid                     33\n",
      "Lodging_Mixed-Humid                   29\n",
      "Mercantile_Mixed-Dry                  24\n",
      "Food Service_Marine                   19\n",
      "Education_Hot-Dry                     18\n",
      "Mercantile_Very Cold                  17\n",
      "Office_Mixed-Dry                      14\n",
      "Warehouse and Storage_Mixed-Dry       13\n",
      "Lodging_Hot-Dry                       11\n",
      "Warehouse and Storage_Very Cold        9\n",
      "Office_Very Cold                       9\n",
      "Education_Marine                       5\n",
      "Education_Very Cold                    5\n",
      "Food Service_Very Cold                 3\n",
      "Lodging_Mixed-Dry                      2\n",
      "Lodging_Marine                         2\n",
      "Food Service_Mixed-Dry                 2\n",
      "Education_Mixed-Dry                    1\n",
      "Education_Subarctic                    1\n",
      "Lodging_Very Cold                      1\n",
      "Name: count, dtype: int64\n",
      "43\n"
     ]
    }
   ],
   "source": [
    "md = pd.read_csv(PATH)\n",
    "# Create a combined column for 'building type' and 'climate zone' to ensure even representation\n",
    "md['building_climate_group'] = md['in.comstock_building_type_group'] + '_' + md['in.building_america_climate_zone']\n",
    "\n",
    "group_counts = md['building_climate_group'].value_counts()\n",
    "print(group_counts)\n",
    "print(len(group_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e3cd52a-5ee9-41b1-a1bc-96ad6c0b7833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def population_split_buildings(path):\n",
    "    # Load metadata\n",
    "    md = pd.read_csv(path)\n",
    "\n",
    "    # Create a combined column for 'building type' and 'climate zone' to ensure even representation\n",
    "    md['building_climate_group'] = md['in.comstock_building_type_group'] + '_' + md['in.building_america_climate_zone']\n",
    "\n",
    "    # Count the number of buildings in each group\n",
    "    group_counts = md['building_climate_group'].value_counts()\n",
    "    target_total = int(0.2*md.shape[0])\n",
    "    target_per_group = target_total // len(group_counts)\n",
    "\n",
    "    # Separate small groups and large groups\n",
    "    small_groups = group_counts[group_counts < target_per_group]\n",
    "    large_groups = group_counts[group_counts >= target_per_group]\n",
    "\n",
    "    # Include all buildings from small groups\n",
    "    small_groups_sampled = md[md['building_climate_group'].isin(small_groups.index)]\n",
    "\n",
    "    # Calculate remaining capacity\n",
    "    remaining_capacity = target_total - small_groups_sampled.shape[0]\n",
    "\n",
    "    # Dynamically sample from large groups\n",
    "    large_groups_sampled = pd.DataFrame()\n",
    "\n",
    "    while remaining_capacity > 0 and not large_groups.empty:\n",
    "        # Number of groups remaining\n",
    "        remaining_groups_count = len(large_groups)\n",
    "\n",
    "        # Determine how many to sample per group\n",
    "        sample_per_large_group = max(remaining_capacity // remaining_groups_count, 1)\n",
    "\n",
    "        # Sample buildings\n",
    "        new_samples = (\n",
    "            md[md['building_climate_group'].isin(large_groups.index)]\n",
    "            .groupby('building_climate_group', group_keys=False)\n",
    "            .apply(lambda x: x.sample(n=min(sample_per_large_group, len(x)), random_state=42))\n",
    "        )\n",
    "\n",
    "        # Add sampled buildings to the final selection\n",
    "        large_groups_sampled = pd.concat([large_groups_sampled, new_samples])\n",
    "\n",
    "        # Update remaining capacity\n",
    "        remaining_capacity -= new_samples.shape[0]\n",
    "\n",
    "        # Update large_groups to exclude groups already fully sampled\n",
    "        sampled_counts = new_samples['building_climate_group'].value_counts()\n",
    "        fully_sampled_groups = sampled_counts[sampled_counts >= group_counts.loc[sampled_counts.index]]\n",
    "        large_groups = large_groups.drop(fully_sampled_groups.index)\n",
    "\n",
    "    # Combine the sampled DataFrames\n",
    "    final_sampled = pd.concat([small_groups_sampled, large_groups_sampled])\n",
    "\n",
    "    # Extract the building IDs and group by building type and climate zone\n",
    "    train_ids = []\n",
    "    test_ids = []\n",
    "\n",
    "    # Group by 'building_climate_group' to ensure balanced distribution\n",
    "    for _, group in final_sampled.groupby('building_climate_group'):\n",
    "        bldg_ids = group['bldg_id']\n",
    "\n",
    "        # Split 80% train, 20% test within each group\n",
    "        if len(bldg_ids) > 1:\n",
    "            train, test = train_test_split(bldg_ids, test_size=0.2, random_state=42)\n",
    "\n",
    "        else:\n",
    "            # If only one building in the group, add it to the train set\n",
    "            train = bldg_ids\n",
    "            test = []\n",
    "\n",
    "        # Append '.csv' to each ID and add to the lists\n",
    "        train_ids.extend([str(x) + '.csv' for x in train])\n",
    "        test_ids.extend([str(x) + '.csv' for x in test])\n",
    "\n",
    "        # # Append '.csv' to each ID and add to the lists\n",
    "        # train_ids.extend(train.astype(str) + '.csv')\n",
    "        # test_ids.extend(test.astype(str) + '.csv')\n",
    "\n",
    "    # Convert to JSON format\n",
    "    split_data = {\n",
    "        \"train_bldg_ids\": train_ids,\n",
    "        \"test_bldg_ids\": test_ids\n",
    "    }\n",
    "\n",
    "    # Save to JSON file\n",
    "    with open('subset20_20_data.json', 'w') as json_file:\n",
    "        json.dump(split_data, json_file, indent=4)\n",
    "\n",
    "    print(\"Building IDs split with '.csv' appended and saved to subset20_20.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd26e6d1-f34f-4797-ad51-ee8ce60fc800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building IDs split with '.csv' appended and saved to subset20_20.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r6/twj1md916gd57zlpnjwmw4p40000gn/T/ipykernel_24207/2961924279.py:37: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(n=min(sample_per_large_group, len(x)), random_state=42))\n",
      "/var/folders/r6/twj1md916gd57zlpnjwmw4p40000gn/T/ipykernel_24207/2961924279.py:37: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(n=min(sample_per_large_group, len(x)), random_state=42))\n",
      "/var/folders/r6/twj1md916gd57zlpnjwmw4p40000gn/T/ipykernel_24207/2961924279.py:37: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(n=min(sample_per_large_group, len(x)), random_state=42))\n"
     ]
    }
   ],
   "source": [
    "population_split_buildings(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920e2ce1-4265-4394-aefb-60f28fb1c997",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fermataVenv",
   "language": "python",
   "name": "fermatavenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
